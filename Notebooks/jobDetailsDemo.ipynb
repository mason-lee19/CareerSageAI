{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"../pinecone/data/db.db\"\n",
    "table_name = \"job_data\"\n",
    "\n",
    "conn = sqlite3.connect(db_name)\n",
    "\n",
    "# Dump the SQL query into a DataFrame\n",
    "df = pd.read_sql_query(f\"\"\"\n",
    "    SELECT jobLink, jobTitle, jobCompany, minSalary, maxSalary, jobDetails, jobLocation, pullDate\n",
    "    FROM {table_name}\n",
    "\"\"\", conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Title: Data Engineer with Pyspark (Onsite)Cognizant (NASDAQ: CTSH) is a leading provider of information technology, consulting, and business process outsourcing services, dedicated to helping the world's leading companies build stronger businesses. Headquartered in Teaneck, New Jersey (U.S.). Cognizant is a member of the NASDAQ-100, the S&P 500, the Forbes Global 1000, and the Fortune 500 and we are among the top performing and fastest growing companies in the world.Practice - AIA - Artificial Intelligence and AnalyticsAbout AI & Analytics: Artificial intelligence (AI) and the data it collects and analyzes will soon sit at the core of all intelligent, human-centric businesses. By decoding customer needs, preferences, and behaviors, our clients can understand exactly what services, products, and experiences their consumers need. Within AI & Analytics, we work to design the future - a future in which trial-and-error business decisions have been replaced by informed choices and data-supported strategies. By applying AI and data science, we help leading companies to prototype, refine, validate, and scale their AI and analytics products and delivery models. Cognizant’s AIA practice takes insights that are buried in data and provides businesses a clear way to transform how they source, interpret, and consume their information. Our clients need flexible data structures and a streamlined data architecture that quickly turns data resources into informative, meaningful intelligence.Location: Mountainview, CA (Onsite)Job SummaryLooking for Data Engineer with Pyspark and AWS skillsResponsibilities1) Requirement understanding and gathering2) Design and develop data pipelines for new module3) Unit testing and help UAT team for any issues4) Upload all the deliverables in Git and help prod team to deploy the code in prodDesign and implement robust data transformation pipelines using pysparkDevelop and maintain a scalable data pipeline architecture that supports a wide variety of data sources and business use cases.Collaborate with data engineers and business stakeholders to define requirements for data transformations and models.Lead best practices for pipeline development including modularity testing version control and continuous integration (CI/CD).Optimize data pipeline models for performance and scalability in a cloud-based data warehouse environment (AWS S3 Redshift).Ensure data quality and governance through well-defined data transformation processes testing frameworks and documentation.Troubleshoot and resolve issues related to data pipelines transformations.Mentor and train team members on data pipeline best practices encouraging a data-driven culture across the organization.Salary and Other Compensation:The annual salary for this position is between $120,000 – $ 130,000 depending on experience and other qualifications of the successful candidate.This position is also eligible for Cognizant’s discretionary annual incentive program, based on performance and subject to the terms of Cognizant’s applicable plans.Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:Medical/Dental/Vision/Life InsurancePaid holidays plus Paid Time Off401(k) plan and contributionsLong-term/Short-term DisabilityPaid Parental LeaveEmployee Stock Purchase PlanDisclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law. Cognizant is an Equal Opportunity Employer M/F/D/V. Cognizant is committed to ensuring that all current and prospective associates are afforded equal opportunities and treatment and a work environment free of harassment. Cognizant is recognized as a Military Friendly Employer and is a coalition member of the Veteran Jobs Mission. Our Cognizant Veterans Network Assists Veterans in building and growing a career at Cognizant that allows them to leverage the leadership, loyalty, integrity, and commitment to excellence instilled in them through participation in military service.We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, sex, gender, gender expression, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform crucial job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_num = 10\n",
    "cur_details = df['jobDetails'].iloc[job_num]\n",
    "df['jobDetails'].iloc[job_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Type: Not specified\n",
      "Is Remote: False\n",
      "Yrs Exp: Not specified\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = text.strip()  # Trim leading and trailing spaces\n",
    "    return text\n",
    "\n",
    "def extract_job_type(text):\n",
    "    job_types = [\"full[- ]?time\", \"part[- ]?time\", \"contract\", \"temporary\", \"internship\", \"freelance\"]\n",
    "    pattern = re.compile(\"|\".join(job_types), re.IGNORECASE)\n",
    "    match = pattern.search(text)\n",
    "    return match.group(0).capitalize() if match else \"Not specified\"\n",
    "\n",
    "def is_remote(text):\n",
    "    remote_keywords = [\"remote\", \"work from home\", \"telecommute\", \"virtual\"]\n",
    "    pattern = re.compile(\"|\".join(remote_keywords), re.IGNORECASE)\n",
    "    return bool(pattern.search(text))\n",
    "\n",
    "def extract_max_years_experience(job_details):\n",
    "    \"\"\"\n",
    "    Extracts the maximum years of experience from a job description.\n",
    "\n",
    "    Parameters:\n",
    "    job_details (str): The job details text blob.\n",
    "\n",
    "    Returns:\n",
    "    str: Maximum years of experience or 'Not specified' if not found.\n",
    "    \"\"\"\n",
    "    # Define regex patterns to capture years of experience\n",
    "    patterns = [\n",
    "        r'(\\d{1,2})\\+?\\s*(?:years|yrs)\\s*of\\s*experience',  # e.g., '5 years of experience', '3+ years'\n",
    "        r'(\\d{1,2})-(\\d{1,2})\\s*(?:years|yrs)',             # e.g., '3-5 years'\n",
    "        r'at least\\s*(\\d{1,2})\\s*(?:years|yrs)'             # e.g., 'at least 2 years'\n",
    "    ]\n",
    "    \n",
    "    # List to collect all found years of experience\n",
    "    years = []\n",
    "\n",
    "    # Loop through patterns and find all matches\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, job_details, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            # If it's a tuple (from a range), take the max of the range\n",
    "            if isinstance(match, tuple):\n",
    "                years.extend(map(int, match))\n",
    "            else:\n",
    "                years.append(int(match))\n",
    "\n",
    "    # Return the max years found or 'Not specified' if the list is empty\n",
    "    return max(years) if years else 'Not specified'\n",
    "\n",
    "clean_details = preprocess_text(cur_details)\n",
    "job_type = extract_job_type(clean_details)\n",
    "is_remote = is_remote(clean_details)\n",
    "yrs_exp = extract_max_years_experience(clean_details)\n",
    "\n",
    "print(\"Job Type:\", job_type)\n",
    "print(\"Is Remote:\", is_remote)\n",
    "print(\"Yrs Exp:\", yrs_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "import json\n",
    "\n",
    "def create_prompt(job_details):\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant that extracts structured information from job postings. Below is a job posting from Indeed:\n",
    "\n",
    "    {job_details}\n",
    "\n",
    "    Extract the following information in JSON format:\n",
    "    - \"job_description\": The full text of the job description, including all responsibilities and skills needed as stated.\n",
    "    - \"requirements\": A list of job requirements exactly as listed in the posting.\n",
    "    - \"company_description\": The full text of the company description (if available).\n",
    "\n",
    "    Maintain the wording and details as much as possible as they are presented. You can summarize lightly.\n",
    "\n",
    "    Return the output in the following JSON format:\n",
    "    {{\n",
    "        \"job_description\": \"...\",\n",
    "        \"requirements\": [\"...\", \"...\"],\n",
    "        \"company_description\": \"...\"\n",
    "    }}\n",
    "\n",
    "    Do not return anything else but the data in JSON format.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def ask_ollama(prompt):\n",
    "    model = OllamaLLM(model=\"llama3.2\")\n",
    "    response = model.invoke(prompt)\n",
    "    return response\n",
    "\n",
    "def parse_llm_response(response):\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def extract_job_details(job_details):\n",
    "    # Preprocess the job details\n",
    "    job_details = preprocess_text(job_details)\n",
    "    \n",
    "    # Create the prompt for Ollama\n",
    "    prompt = create_prompt(job_details)\n",
    "    \n",
    "    # Ask Ollama to extract the information\n",
    "    response = ask_ollama(prompt)\n",
    "    \n",
    "    # Parse the response into a dictionary\n",
    "    structured_data = parse_llm_response(response)\n",
    "    \n",
    "    return structured_data\n",
    "\n",
    "extracted_sections = extract_job_details(clean_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description: Looking for Data Engineer with Pyspark and AWS skillsResponsibilities1) Requirement understanding and gathering2) Design and develop data pipelines for new module3) Unit testing and help UAT team for any issues4) Upload all the deliverables in Git and help prod team to deploy the code in prodDesign and implement robust data transformation pipelines using pysparkDevelop and maintain a scalable data pipeline architecture that supports a wide variety of data sources and business use cases.Collaborate with data engineers and business stakeholders to define requirements for data transformations and models.Lead best practices for pipeline development including modularity testing version control and continuous integration (CI/CD).Optimize data pipeline models for performance and scalability in a cloud-based data warehouse environment (AWS S3 Redshift).Ensure data quality and governance through well-defined data transformation processes testing frameworks and documentation.Troubleshoot and resolve issues related to data pipelines transformations.Mentor and train team members on data pipeline best practices encouraging a data-driven culture across the organization.Salary and Other Compensation:The annual salary for this position is between $120,000 – $ 130,000 depending on experience and other qualifications of the successful candidate.This position is also eligible for Cognizant’s discretionary annual incentive program, based on performance and subject to the terms of Cognizant’s applicable plans.Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:Medical/Dental/Vision/Life InsurancePaid holidays plus Paid Time Off401(k) plan and contributionsLong-term/Short-term DisabilityPaid Parental LeaveEmployee Stock Purchase PlanDisclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law. Cognizant is an Equal Opportunity Employer M/F/D/V. Cognizant is committed to ensuring that all current and prospective associates are afforded equal opportunities and treatment and a work environment free of harassment. Cognizant is recognized as a Military Friendly Employer and is a coalition member of the Veteran Jobs Mission. Our Cognizant Veterans Network Assists Veterans in building and growing a career at Cognizant that allows them to leverage the leadership, loyalty, integrity, and commitment to excellence instilled in them through participation in military service.We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, sex, gender, gender expression, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform crucial job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n",
      "********************\n",
      "requirements: ['Requirement understanding and gathering', 'Design and develop data pipelines for new module', 'Unit testing and help UAT team for any issues', 'Upload all the deliverables in Git and help prod team to deploy the code in prod', 'Design and implement robust data transformation pipelines using pyspark', 'Develop and maintain a scalable data pipeline architecture that supports a wide variety of data sources and business use cases.', 'Collaborate with data engineers and business stakeholders to define requirements for data transformations and models.', 'Lead best practices for pipeline development including modularity testing version control and continuous integration (CI/CD).', 'Optimize data pipeline models for performance and scalability in a cloud-based data warehouse environment (AWS S3 Redshift).', 'Ensure data quality and governance through well-defined data transformation processes testing frameworks and documentation.', 'Troubleshoot and resolve issues related to data pipelines transformations.', 'Mentor and train team members on data pipeline best practices encouraging a data-driven culture across the organization.']\n",
      "********************\n",
      "Company Description: Cognizant is a leading provider of information technology, consulting, and business process outsourcing services, dedicated to helping the world's leading companies build stronger businesses. Headquartered in Teaneck, New Jersey (U.S.). Cognizant is a member of the NASDAQ-100, the S&P 500, the Forbes Global 1000, and the Fortune 500 and we are among the top performing and fastest growing companies in the world.Practice - AIA - Artificial Intelligence and AnalyticsAbout AI & Analytics: Artificial intelligence (AI) and the data it collects and analyzes will soon sit at the core of all intelligent, human-centric businesses. By decoding customer needs, preferences, and behaviors, our clients can understand exactly what services, products, and experiences their consumers need. Within AI & Analytics, we work to design the future - a future in which trial-and-error business decisions have been replaced by informed choices and data-supported strategies. By applying AI and data science, we help organizations make sense of their data to gain valuable insights that can drive business value. Our AI and Analytics practice is built around four pillars: Machine Learning, Advanced Data Science, Natural Language Processing, and Advanced Analytics.\n"
     ]
    }
   ],
   "source": [
    "print(\"Job Description:\",extracted_sections.get(\"job_description\",\"\"))\n",
    "print(\"*\"*20)\n",
    "print(\"requirements:\",extracted_sections.get(\"requirements\",\"\"))\n",
    "print(\"*\"*20)\n",
    "print(\"Company Description:\",extracted_sections.get(\"company_description\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 ('py309Venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "094ffe61415a64c96c389b2ac3d6e28f3d4de70e9bd4a77bdc227cc709cf2d5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
