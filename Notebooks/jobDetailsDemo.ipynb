{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"../pinecone/data/db.db\"\n",
    "table_name = \"job_data\"\n",
    "\n",
    "conn = sqlite3.connect(db_name)\n",
    "\n",
    "# Dump the SQL query into a DataFrame\n",
    "df = pd.read_sql_query(f\"\"\"\n",
    "    SELECT jobLink, jobTitle, jobCompany, minSalary, maxSalary, jobDetails, jobLocation, pullDate\n",
    "    FROM {table_name}\n",
    "\"\"\", conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DELETE a table\n",
    "\"\"\"\n",
    "\n",
    "db_name = \"../pinecone/data/db.db\"\n",
    "table_name = \"ai_desc_data_v4\"\n",
    "\n",
    "conn = sqlite3.connect(db_name)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"../pinecone/data/db.db\"\n",
    "table_name = \"ai_desc_data\"\n",
    "\n",
    "conn = sqlite3.connect(db_name)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Dump the SQL query into a DataFrame\n",
    "df = pd.read_sql_query(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {table_name}\n",
    "\"\"\", conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jobLink        https://www.indeed.com/rc/clk?jk=21f38ec02c730...\n",
       "jobTitle                              Data Engineer (L5) - Games\n",
       "jobCompany                                               Netflix\n",
       "minSalary                                                170,000\n",
       "maxSalary                                                720,000\n",
       "jobDetails     Job Requisition ID     JR29190     Job Posting...\n",
       "jobLocation                                        Los Gatos, CA\n",
       "pullDate                                              2024-10-23\n",
       "Name: 8, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "            You are a helpful assistant that extracts structured information from job postings. Below is a job posting from Indeed:\n",
    "\n",
    "            {df.iloc[8][\"jobDetails\"]}\n",
    "\n",
    "            Extract the following information in JSON format:\n",
    "            - \"job_description\": A reworded and slightly summarized version of the full job description. Include all responsibilities, required skills, and key tasks. If the job description is not clearly labeled, infer it from any relevant sections, such as role expectations, day-to-day activities, or skill requirements. Avoid leaving this field blank unless absolutely no relevant information is present. Avoid exact wording.\n",
    "            - \"requirements\": A rephrased list of job requirements, maintaining the meaning but avoiding exact wording.\n",
    "            - \"experience\": A string with the minimum years of experience required for this job, formatted like \"3 Years\" or \"1 Year\".\n",
    "            - \"company_description\": A summarized and reworded version of the company description, if available.\n",
    "\n",
    "            Maintain accuracy while avoiding direct copying of phrases from the original text. The rephrasing should provide the same essential information in a fresh and unique way.\n",
    "\n",
    "\n",
    "            Return the output in the following JSON format:\n",
    "            {{\n",
    "                \"job_description\": \"...\",\n",
    "                \"requirements\": [\"...\", \"...\"],\n",
    "                \"experience\": \"...\",\n",
    "                \"company_description\": \"...\"\n",
    "            }}\n",
    "\n",
    "            Do not return anything else but the data in JSON format.\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "API_URL = \"http://192.168.1.27:11434/api/chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"model\":\"llama3.2\",\n",
    "    \"messages\": [\n",
    "        {\"role\":\"user\",\n",
    "        \"content\":prompt}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(API_URL,json=payload,stream=True)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error {response.status_code} : {response.text}\")\n",
    "\n",
    "full_message = \"\"\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        try:\n",
    "            message_data = json.loads(line)\n",
    "            if \"message\" in message_data and \"content\" in message_data[\"message\"]:\n",
    "                full_message += message_data[\"message\"][\"content\"]\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error Decoding JSON: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job_description': 'The Data Engineer will build robust data pipelines to output high-quality data at scale, collaborating with stakeholders to develop software and data solutions for analysis across the entire library of games. They will also work closely with the logging infrastructure team to establish key schemas and manage cross-functional engineering projects with autonomy. Responsibilities include increasing automation and scalability of complex data sets and building production data pipelines using Spark, Flink, or Hive/Hadoop.',\n",
       " 'requirements': ['7+ years in software development with experience in data processing for analytics',\n",
       "  'high-level autonomy in managing projects',\n",
       "  'experience with schema design and data modeling',\n",
       "  'programming proficiency in Python, Scala, or Java',\n",
       "  'strong SQL skills',\n",
       "  'excellent communication skills'],\n",
       " 'experience': '7 Years',\n",
       " 'company_description': 'Netflix is a leading entertainment service provider with over 250 million paid members worldwide. The Data Science and Engineering team aims to improve business aspects using data analytics and sciences.'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = json.loads(full_message)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"Requirement understanding and gathering\", \"Design and develop data pipelines for new module\", \"Unit testing and help UAT team for any issues\", \"Upload all the deliverables in Git and help prod team to deploy the code in prod\", \"Design and implement robust data transformation pipelines using pyspark\", \"Develop and maintain a scalable data pipeline architecture that supports a wide variety of data sources and business use cases.\", \"Collaborate with data engineers and business stakeholders to define requirements for data transformations and models.\", \"Lead best practices for pipeline development including modularity testing version control and continuous integration (CI/CD).\", \"Optimize data pipeline models for performance and scalability in a cloud-based data warehouse environment (AWS S3 Redshift).\", \"Ensure data quality and governance through well-defined data transformation processes testing frameworks and documentation.\", \"Troubleshoot and resolve issues related to data pipelines transformations.\", \"Mentor and train team members on data pipeline best practices encouraging a data-driven culture across the organization.\"]'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['requirements'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job Description:Resources will assist clients with rack, stack and installation of key data center equipment.They will help troubleshoot common issues, work through various tickets and act as a level 1 administrator for any data center issues.Do mechanical assembly/disassembly and install heavy GPU trays into systemsBuild up prototype open board systems on the benchtop and debugging issues reported by users.Triage and debug systems and get systems to pass base line diagnostics.Requirements:Strong data center experience with rack and stack,Minimum of 5 years on-prem large scale data center experience supporting the Data Center equipment and/or infrastructure (Power, Space, Cooling, Equipment)Strong cabling skills, cable management and dressingStrong NW troubleshooting experience, ideally with Supermicro Servers, Infiniband Fabric and Mellanox, Arista HardwareStrong attention to detail, independent and out of box thinkingSolid skills in UNIX (Ubuntu or RedHat) administration and knowledge of commands including logging into the server’s to monitor for disk failures, debug systems, etcBreak fix server expertise including mechanical assembly/disassembly, installing heavy GPU trays, changing system configs and building up prototypeJob Type: Full-timePay: $40.00 - $65.00 per hourBenefits:401(k)Dental insuranceHealth insurancePaid time offVision insuranceSchedule:Day shiftExperience:Data center: 2 years (Required)Cabling: 2 years (Required)Ability to Relocate:San Jose, CA 95110: Relocate before starting work (Required)Work Location: In person'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_num = 17\n",
    "cur_details = df['jobDetails'].iloc[job_num]\n",
    "cur_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Type: Full-time\n",
      "Is Remote: False\n",
      "Yrs Exp: Not specified\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = text.strip()  # Trim leading and trailing spaces\n",
    "    return text\n",
    "\n",
    "def extract_job_type(text):\n",
    "    job_types = [\"full[- ]?time\", \"part[- ]?time\", \"contract\", \"temporary\", \"internship\", \"freelance\"]\n",
    "    pattern = re.compile(\"|\".join(job_types), re.IGNORECASE)\n",
    "    match = pattern.search(text)\n",
    "    return match.group(0).capitalize() if match else \"Not specified\"\n",
    "\n",
    "def is_remote(text):\n",
    "    remote_keywords = [\"remote\", \"work from home\", \"telecommute\", \"virtual\"]\n",
    "    pattern = re.compile(\"|\".join(remote_keywords), re.IGNORECASE)\n",
    "    return bool(pattern.search(text))\n",
    "\n",
    "def extract_max_years_experience(job_details):\n",
    "    \"\"\"\n",
    "    Extracts the maximum years of experience from a job description.\n",
    "\n",
    "    Parameters:\n",
    "    job_details (str): The job details text blob.\n",
    "\n",
    "    Returns:\n",
    "    str: Maximum years of experience or 'Not specified' if not found.\n",
    "    \"\"\"\n",
    "    # Define regex patterns to capture years of experience\n",
    "    patterns = [\n",
    "        r'(\\d{1,2})\\+?\\s*(?:years|yrs)\\s*of\\s*experience',  # e.g., '5 years of experience', '3+ years'\n",
    "        r'(\\d{1,2})-(\\d{1,2})\\s*(?:years|yrs)',             # e.g., '3-5 years'\n",
    "        r'at least\\s*(\\d{1,2})\\s*(?:years|yrs)'             # e.g., 'at least 2 years'\n",
    "    ]\n",
    "    \n",
    "    # List to collect all found years of experience\n",
    "    years = []\n",
    "\n",
    "    # Loop through patterns and find all matches\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, job_details, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            # If it's a tuple (from a range), take the max of the range\n",
    "            if isinstance(match, tuple):\n",
    "                years.extend(map(int, match))\n",
    "            else:\n",
    "                years.append(int(match))\n",
    "\n",
    "    # Return the max years found or 'Not specified' if the list is empty\n",
    "    return max(years) if years else 'Not specified'\n",
    "\n",
    "clean_details = preprocess_text(cur_details)\n",
    "job_type = extract_job_type(clean_details)\n",
    "is_remote = is_remote(clean_details)\n",
    "yrs_exp = extract_max_years_experience(clean_details)\n",
    "\n",
    "print(\"Job Type:\", job_type)\n",
    "print(\"Is Remote:\", is_remote)\n",
    "print(\"Yrs Exp:\", yrs_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "import json\n",
    "\n",
    "def create_prompt(job_details):\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant that extracts structured information from job postings. Below is a job posting from Indeed:\n",
    "\n",
    "    {job_details}\n",
    "\n",
    "    Extract the following information in JSON format:\n",
    "    - \"job_description\": The full text of the job description, including all responsibilities and skills needed as stated.\n",
    "    - \"requirements\": A list of job requirements exactly as listed in the posting.\n",
    "    - \"employment_type\": A string with either full-time or contract if the job is full-time or contract.\n",
    "    - \"experience\": A string of the range of amount of years required for this job, return None if job posting does not mention.\n",
    "    - \"remote\": A boolean, 1 if the job is remote, 0 if the job is not remote.\n",
    "    - \"company_description\": The full text of the company description (if available).\n",
    "\n",
    "    Maintain the wording and details as much as possible as they are presented. You can summarize lightly.\n",
    "\n",
    "    Return the output in the following JSON format:\n",
    "    {{\n",
    "        \"job_description\": \"...\",\n",
    "        \"requirements\": [\"...\", \"...\"],\n",
    "        \"employment_type\": \"...\",\n",
    "        \"experience\": \"...\",\n",
    "        \"remote\": \"...\",\n",
    "        \"company_description\": \"...\"\n",
    "    }}\n",
    "\n",
    "    Do not return anything else but the data in JSON format.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def ask_ollama(prompt):\n",
    "    model = OllamaLLM(model=\"llama3.2\")\n",
    "    response = model.invoke(prompt)\n",
    "    return response\n",
    "\n",
    "def parse_llm_response(response):\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def extract_job_details(job_details):\n",
    "    # Preprocess the job details\n",
    "    job_details = preprocess_text(job_details)\n",
    "    \n",
    "    # Create the prompt for Ollama\n",
    "    prompt = create_prompt(job_details)\n",
    "    \n",
    "    # Ask Ollama to extract the information\n",
    "    response = ask_ollama(prompt)\n",
    "    \n",
    "    # Parse the response into a dictionary\n",
    "    structured_data = parse_llm_response(response)\n",
    "    \n",
    "    return structured_data\n",
    "\n",
    "extracted_sections = extract_job_details(clean_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description: Resources will assist clients with rack, stack and installation of key data center equipment.They will help troubleshoot common issues, work through various tickets and act as a level 1 administrator for any data center issues.Do mechanical assembly/disassembly and install heavy GPU trays into systemsBuild up prototype open board systems on the benchtop and debugging issues reported by users.Triage and debug systems and get systems to pass base line diagnostics.\n",
      "********************\n",
      "requirements: ['Strong data center experience with rack and stack', 'Minimum of 5 years on-prem large scale data center experience supporting the Data Center equipment and/or infrastructure (Power, Space, Cooling, Equipment)', 'Strong cabling skills, cable management and dressing', 'Strong NW troubleshooting experience, ideally with Supermicro Servers, Infiniband Fabric and Mellanox, Arista Hardware', 'Strong attention to detail, independent and out of box thinking', 'Solid skills in UNIX (Ubuntu or RedHat) administration and knowledge of commands including logging into the server’s to monitor for disk failures, debug systems, etc', 'Break fix server expertise including mechanical assembly/disassembly, installing heavy GPU trays, changing system configs and building up prototype']\n",
      "********************\n",
      "exmployment Type: Full-time\n",
      "********************\n",
      "Experience: 2 years\n",
      "********************\n",
      "remote: False\n",
      "********************\n",
      "Company Description: \n"
     ]
    }
   ],
   "source": [
    "print(\"Job Description:\",extracted_sections.get(\"job_description\",\"\"))\n",
    "print(\"*\"*20)\n",
    "print(\"requirements:\",extracted_sections.get(\"requirements\",\"\"))\n",
    "print(\"*\"*20)\n",
    "print(\"exmployment Type:\",extracted_sections.get(\"employment_type\",\"\"))\n",
    "print(\"*\"*20)\n",
    "print(\"Experience:\",extracted_sections.get(\"experience\",\"\"))\n",
    "print(\"*\"*20)\n",
    "print(\"remote:\",extracted_sections.get(\"remote\",\"\"))\n",
    "print(\"*\"*20)\n",
    "print(\"Company Description:\",extracted_sections.get(\"company_description\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310Venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
