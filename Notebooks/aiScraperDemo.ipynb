{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements:\n",
    "langchain <br>\n",
    "langchain_ollama <br>\n",
    "selenium <br>\n",
    "beaufifulsoup4 <br>\n",
    "lxml <br>\n",
    "html5lib <br>\n",
    "python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium.webdriver as webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_website(website):\n",
    "    print(\"Launching chrome browser...\")\n",
    "\n",
    "    chrome_driver_path = '../utils/chromedriver'\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(website)\n",
    "        print(\"Page loaded...\")\n",
    "        html = driver.page_source\n",
    "\n",
    "        return html\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def extract_body_content(html_content):\n",
    "    soup = BeautifulSoup(html_content,\"html.parser\")\n",
    "    body_content = soup.body\n",
    "\n",
    "    if body_content:\n",
    "        print(\"Content extracted...\")\n",
    "        return str(body_content)\n",
    "    \n",
    "    print(\"Unable to collect content\")\n",
    "    \n",
    "def clean_body_content(body_content):\n",
    "    soup = BeautifulSoup(body_content,\"html.parser\")\n",
    "\n",
    "    for script_or_style in soup([\"script\",\"style\"]):\n",
    "        script_or_style.extract()\n",
    "\n",
    "    cleaned_content = soup.get_text(separator='\\n')\n",
    "    cleaned_content = \"\\n\".join(line.strip() for line in cleaned_content.splitlines() if line.strip())\n",
    "\n",
    "    print(\"Content cleaned...\")\n",
    "    return cleaned_content\n",
    "\n",
    "def split_dom_content(dom_content,max_length=6000):\n",
    "    return [\n",
    "        dom_content[i:i+max_length] for i in range(0,len(dom_content),max_length)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once Ollama is downloaded just do a 'ollama pull 'model-name'' to pull and be able to utilize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = OllamaLLM(model='llama3')\n",
    "\n",
    "template = (\n",
    "    \"You are tasked with extracting specific information from the following text content: {dom_content}. \"\n",
    "    \"Please follow these instructions carefully: \\n\\n\"\n",
    "    \"1. **Extract Information:** Only extract the information that directly matches the provided description: {parse_description}. \"\n",
    "    \"2. **No Extra Content:** Do not include any additional text, comments, or explanations in your response. \"\n",
    "    \"3. **Empty Response:** If no information matches the description, return an empty string ('').\"\n",
    "    \"4. **Direct Data Only:** Your output should contain only the data that is explicitly requested, with no other text.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_with_ollama(dom_chunks,parse_description):\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model\n",
    "\n",
    "    parsed_results = []\n",
    "\n",
    "    for i,chunk in enumerate(dom_chunks,start=1):\n",
    "        response = chain.invoke({\"dom_content\":chunk, \"parse_description\":parse_description})\n",
    "        print(f\"Parsed batch {i} of {len(dom_chunks)}\")\n",
    "\n",
    "        parsed_results.append(response)\n",
    "\n",
    "    return \"\\n\".join(parsed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://jobs.apple.com/en-us/search?key=data%252520engineer&location=united-states-USA&sort=relevance&page=2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make this work you have to download the chrome driver and also run it in terminal 'pwd'/chromedriver and then when it pops up with a warning you open it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching chrome browser...\n",
      "Page loaded...\n",
      "Unable to collect content\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m scrape_website(url)\n\u001b[1;32m      2\u001b[0m body_content \u001b[38;5;241m=\u001b[39m extract_body_content(result)\n\u001b[0;32m----> 3\u001b[0m cleaned_content \u001b[38;5;241m=\u001b[39m \u001b[43mclean_body_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m dom_chunks \u001b[38;5;241m=\u001b[39m split_dom_content(cleaned_content)\n",
      "Cell \u001b[0;32mIn[54], line 29\u001b[0m, in \u001b[0;36mclean_body_content\u001b[0;34m(body_content)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_body_content\u001b[39m(body_content):\n\u001b[0;32m---> 29\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody_content\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m script_or_style \u001b[38;5;129;01min\u001b[39;00m soup([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m     32\u001b[0m         script_or_style\u001b[38;5;241m.\u001b[39mextract()\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/py310Venv/lib/python3.10/site-packages/bs4/__init__.py:315\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(markup, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):        \u001b[38;5;66;03m# It's a file-type object.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     markup \u001b[38;5;241m=\u001b[39m markup\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    316\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[1;32m    318\u001b[0m ):\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# Issue warnings for a couple beginner problems\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# involving passing non-markup to Beautiful Soup.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;66;03m# Beautiful Soup will still parse the input as markup,\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# since that is sometimes the intended behavior.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_is_url(markup):\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_resembles_filename(markup)                \n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "result = scrape_website(url)\n",
    "body_content = extract_body_content(result)\n",
    "cleaned_content = clean_body_content(body_content)\n",
    "dom_chunks = split_dom_content(cleaned_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed batch 1 of 6\n",
      "Parsed batch 2 of 6\n",
      "Parsed batch 3 of 6\n",
      "Parsed batch 4 of 6\n",
      "Parsed batch 5 of 6\n",
      "Parsed batch 6 of 6\n"
     ]
    }
   ],
   "source": [
    "res = parse_with_ollama(dom_chunks,'Extract all relevent job data including job title, job link, salary range, location, team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'US-Business Expert\\nSales and Business Development\\nDec 10, 2024\\nVarious Locations within United States\\nHere is the extracted information:\\n\\n**US-Business Pro:**\\n* Role Number: 200125453\\n* Weekly Hours: 40 Hours\\n\\n**US-Genius:**\\n* Role Number: 114438151\\n* Weekly Hours: 40 Hours\\n\\n**US-Creative:**\\n* Role Number: 114438149\\n* Weekly Hours: 40 Hours\\n\\n**US-Operations Expert:**\\n* Role Number: 114438152\\n* Weekly Hours: 40 Hours\\n\\n**US-Expert:**\\n* Role Number: 114438150\\n* Weekly Hours: 40 Hours\\nHere are the extracted job details:\\n\\n**US-Expert**\\n\\n* Role Number: N/A\\n* Weekly Hours: N/A\\n\\n**US-Technical Specialist**\\n\\n* Role Number: 114438201\\n* Weekly Hours: 40 Hours\\n\\n**US - Specialist: Full-Time, Part-Time, and Part-Time Temporary**\\n\\n* Role Number: 114438158\\n* Weekly Hours: 40 Hours\\n\\n**WatchOS Software QA Engineer**\\n\\n* Role Number: 200576662\\n* Weekly Hours: 40 Hours\\n\\n**Core Bringup Engineering Manager**\\n\\n* Role Number: 200582675\\n* Weekly Hours: 40 Hours\\n\\n**Software Tools Development Engineer - CoreOS**\\n\\n* Role Number: 200582786\\n* Weekly Hours: 40 Hours\\nHere is the extracted information:\\n\\n* Job Title: Software Tools Development Engineer - CoreOS\\n* Role Description: Building tools and infrastructure to qualify Operating System and related technologies at Apple for various product platforms.\\n* Location: Mesa\\n* Weekly Hours: 40 Hours\\nHere are the extracted job details:\\n\\n**Job 1: Product Designer**\\n\\n* Role Number: 200567683\\n* Weekly Hours: 40 Hours\\n* Job Title: Product Designer\\n* Location: Sunnyvale\\n* Date: Dec 10, 2024\\n\\n**Job 2: WiFi Connectivity Validation Engineer**\\n\\n* Role Number: 200573862\\n* Weekly Hours: 40 Hours\\n* Job Title: WiFi Connectivity Validation Engineer\\n* Location: Sunnyvale\\n* Date: Dec 10, 2024\\n\\n**Job 3: Wireless System Validation Engineer**\\n\\n* Role Number: 200573998\\n* Weekly Hours: 40 Hours\\n* Job Title: Wireless System Validation Engineer\\n* Location: Sunnyvale\\n* Date: Dec 10, 2024\\n\\n**Job 4: Wireless System Automation and Test Engineer**\\n\\n* Role Number: 200573958\\n* Weekly Hours: 40 Hours\\n* Job Title: Wireless System Automation and Test Engineer\\n* Location: Irvine\\n* Date: Dec 10, 2024\\nor retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310Venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
