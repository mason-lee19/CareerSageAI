{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from curl_cffi import requests as cureq\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import time\n",
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "class JobListing(BaseModel):\n",
    "    jobLink: List[str]\n",
    "    jobTitle: List[str]\n",
    "    jobCompany: List[str]\n",
    "    minSalary: List[str]\n",
    "    maxSalary: List[str]\n",
    "    jobDetails: List[str]\n",
    "\n",
    "def pull_job_details(resp):\n",
    "    job_list = {'jobLink':[],'jobTitle':[],'jobCompany':[],'minSalary':[],'maxSalary':[],'jobDetails':[]}\n",
    "\n",
    "    if 'text/html' in resp.headers['Content-Type'] and resp.status_code == 200:\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "        outer_most_point=soup.find('div',attrs={'id': 'mosaic-provider-jobcards'})\n",
    "\n",
    "        for i in outer_most_point.find('ul'):\n",
    "            a = i.find('a')\n",
    "            if not a:\n",
    "                continue\n",
    "\n",
    "            href_link = a.get('href')\n",
    "            job_list['jobLink'].append(href_link)\n",
    "\n",
    "            job_link = 'https://www.indeed.com' + href_link\n",
    "            job_salary,job_description = pull_job_desc(job_link)\n",
    " \n",
    "            if job_salary != 'Not Specified' and len(job_salary.split(' ')) > 2:\n",
    "                job_list['minSalary'].append(job_salary.split('-')[0].replace('$',''))\n",
    "                job_list['maxSalary'].append(job_salary.split('-')[1].split(' ')[1].replace('$',''))\n",
    "            else:\n",
    "                job_list['minSalary'].append('None')\n",
    "                job_list['maxSalary'].append('None')\n",
    "\n",
    "            job_list['jobDetails'].append(job_description)\n",
    "\n",
    "            raw_title = i.find('span',id=lambda x: x and x.startswith('jobTitle-'))\n",
    "            if raw_title:\n",
    "                job_list['jobTitle'].append(raw_title.get_text())\n",
    "            else:\n",
    "                job_list['jobTitle'].append('None')\n",
    "\n",
    "            raw_company = i.find('span',{'data-testid':'company-name'})\n",
    "            if raw_company:\n",
    "                job_list['jobCompany'].append(raw_company.get_text())\n",
    "            else:\n",
    "                job_list['jobCompany'].append('None')\n",
    "\n",
    "    return job_list\n",
    "\n",
    "def pull_job_desc(job_link):\n",
    "    resp = cureq.get(job_link,impersonate='chrome')\n",
    "\n",
    "    if 'text/html' in resp.headers['Content-Type'] and resp.status_code == 200:\n",
    "\n",
    "        soup = BeautifulSoup(resp.text,'html.parser')\n",
    "        outer_most_points = soup.find('div',class_=re.compile(r'^fastviewjob'))\n",
    "\n",
    "        raw_salary = outer_most_points.find('div',attrs={'id':'salaryInfoAndJobType'})\n",
    "        salary = 'Not Specified'\n",
    "        \n",
    "        if raw_salary:\n",
    "            salary = raw_salary.get_text()\n",
    "        \n",
    "        raw_description = outer_most_points.find('div',attrs={'id':'jobDescriptionText'})\n",
    "        description = 'None'\n",
    "        \n",
    "        if raw_description:\n",
    "            description = raw_description.get_text().replace('\\n','')\n",
    "        \n",
    "    return salary,description\n",
    "\n",
    "def format_search(search):\n",
    "    return search.replace(' ','+')\n",
    "\n",
    "def new_session():\n",
    "    session = cureq.Session(impersonate=\"chrome\",proxy=os.getenv(\"stickyproxy\"))\n",
    "    return session\n",
    "\n",
    "def search_api(session: cureq.Session, job_title: str, location:str, start_num: int):\n",
    "    url = f\"https://www.indeed.com/jobs?q={format_search(job_title)}&l={format_search(location)}%2C++CA&start={str(start_num)}\"\n",
    "    resp = session.get(url,headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    return JobListing(**pull_job_details(resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use loop to pull all current job listings with a search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in range(0,3):\n",
    "    listing = search_api(new_session(),'data+engineer','mountain+view',i*10)\n",
    "    df = pd.concat([df,pd.DataFrame(listing.dict())],ignore_index=True)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_output(text:str):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n','    ')\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def pull_skills(resp):\n",
    "    if 'text/html' in resp.headers['Content-Type'] and resp.status_code == 200:\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        print(soup)\n",
    "\n",
    "        # Find all button elements with 'data-testid' that ends with '-tile'\n",
    "        buttons = soup.find_all('button', {'data-testid': lambda x: x and x.endswith('-tile')})\n",
    "        # Extract the tile name from the 'data-testid' attribute\n",
    "        tile_names = [button['data-testid'].replace('-tile', '') for button in buttons]\n",
    "\n",
    "        for name in tile_names:\n",
    "            print(name)\n",
    "\n",
    "def pull_job_data(resp):\n",
    "    if 'text/html' in resp.headers['Content-Type'] and resp.status_code == 200:\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "        desc = soup.find('div',{'id':'jobDescriptionTitle'})\n",
    "\n",
    "        if not desc:\n",
    "            print('No job description found')\n",
    "            return None\n",
    "        \n",
    "        return desc.next_sibling.get_text()\n",
    "        \n",
    "def get_raw_text(resp):\n",
    "    soup = BeautifulSoup(resp.text,'html.parser')\n",
    "    return clean_output(soup.get_text())\n",
    "\n",
    "'''Need to make the scraper more complex to only pull pertanent information.\n",
    "Currently pulling everything and don't want to affect the relevence search\n",
    "'''\n",
    "def scrape_webpage_text(session: cureq.Session, job_link: str):\n",
    "    url = f'https://www.indeed.com{job_link}'\n",
    "    print(url)\n",
    "    resp = session.get(url,headers=headers)\n",
    "    \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use fpdf to pull job postings and put into pdf to save for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/rc/clk?jk=91f171f8260d5452&bb=Eb6IIhKxqsmRoSAWiE6m87Prf-WFe5sbpIlTHZuA5Sfr5JdZAT2ElTyf_2CHtVYCr-_DZ3j_fbHmBg8XTYFSIzGHpan297JZ9aZwI-7ZdKKWMVn-M4JGnw%3D%3D&xkcb=SoCI67M36IqRSDS00Z0LbzkdCdPP&fccid=795688f508690d17&vjs=3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\",size=6)\n",
    "resp = scrape_webpage_text(new_session(),df.iloc[0]['jobLink'])\n",
    "text = clean_output(pull_job_data(resp))\n",
    "pdf.multi_cell(0,4,txt=text)\n",
    "pdf.output(\"../data/test.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310Venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
